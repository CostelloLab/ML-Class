---
title: "CPBS 7630 - Module 1"
date: "January 30, 2018"
output: html_document
---

---

#### Contents:

* [Accessing Synapse data](#synapse)
* [Cleaning data](#cleaning)
* [Feature selection](#feature_selection)
* [Data imputation](#imputation)
* [Machine learning](#ml)
* [Evaluation](#evaluation)

---

<!-- <a name="synapse"/> -->

<!-- ### Accessing Synapse data -->

<!-- For this course, we will be using drug sensitivity data from [Costello, et al. A Community Effort to Assess and Improve Drug Sensitivity Prediction Algorithms (2014)](http://www.nature.com/nbt/journal/v32/n12/full/nbt.2877.html). This [data set](https://www.synapse.org/#!Synapse:syn2785783) consists of gene expression measurements for 44 breast cancer cell lines, and the dose-response values of growth inhibition for each cell line exposed to 28 therapeutic compounds.  -->

<!-- ```{r, echo = F, message = F} -->
<!-- library(synapseClient) -->

<!-- # Log in to Synapse -->
<!-- #synapseLogin(username = 'myusername', password = 'mypassword') -->
<!-- ``` -->

<!-- ```{r, message = F} -->

<!-- library(synapseClient) -->
<!-- synapseLogin(username = 'james.costello', password = 'Hammer70m$') -->

<!-- # Get gene expression data from the DREAM7 challenge -->
<!-- DREAM7.expression.data <- synGet('syn2785861') -->
<!-- local.file.path <- DREAM7.expression.data@filePath -->
<!-- expression.data <- read.delim(local.file.path,  -->
<!--                               header = T,  -->
<!--                               stringsAsFactors = F, -->
<!--                               check.names = F) -->

<!-- # Raw expression data consists of cell lines and genes -->
<!-- dim(expression.data) -->

<!-- # Get the drug response data from Synpase, training and test sets -->
<!-- DREAM7.train <- synGet('syn2785850') -->
<!-- train.data <- read.table(getFileLocation(DREAM7.train),  -->
<!--                          header = T,  -->
<!--                          sep='\t',  -->
<!--                          row.names = 1, -->
<!--                          stringsAsFactors = F) -->

<!-- DREAM7.test <- synGet('syn2785837') -->
<!-- test.data <- read.table(getFileLocation(DREAM7.test), -->
<!--                         header = T, -->
<!--                         sep='\t',  -->
<!--                         row.names = 1,  -->
<!--                         stringsAsFactors = F) -->

<!-- # Concatenate the drug response data for train and test set -->
<!-- drug.response <- rbind(test.data,train.data) -->

<!-- # Pull out data on one drug (Drug 7) to use for the exercise in the last section -->
<!-- drug7.response <- drug.response$Drug7 -->
<!-- names(drug7.response) = row.names(drug.response) -->

<!-- ``` -->

<a name="cleaning"/>

### Reading in DS data

``` {r}
library(tidyverse)
library(ggplot2)

comorbidities <- read.delim("../Projects/Code/DS-transfer/data/P4C_Comorbidity_020921.tsv", skip = 1)
head(comorbidities)

metadata <-read.delim("../Projects/Code/DS-transfer/data/P4C_metadata_021921_Costello.txt")
head(metadata)

comorbidities <- comorbidities %>%
    select(RecordID, Condition, HasCondition) %>%
    pivot_wider(names_from = Condition, values_from = HasCondition)


meta <- metadata %>%
    inner_join(comorbidities, by = "RecordID")
head(meta)

expression.profiles <- read.delim("../Projects/Code/DS-transfer/data/HTP_WholeBlood_RNAseq_Counts_Synapse.txt")
expression.data <- expression.profiles %>%
    select( LabID, EnsemblID,Value) %>%
    group_by(LabID) %>%
    mutate(row = row_number())%>%
    pivot_wider( names_from = c("EnsemblID", "row"), values_from = Value)%>%
    setNames(sub("_[^_]+$", "",names(.))) %>%
    column_to_rownames(var = "LabID")
    

dim(expression.data)

```


### Cleaning data

Some common steps for data cleaning include checking for duplicated names, converting your data into matrix form, and performing any necessary normalization or transformation. 

```{r}

# Store gene names for later, remove non-numeric data
## genes <- expression.data$HGNC_ID
## expression.data$Ensembl_ID <- NULL
## expression.data$HGNC_ID <- NULL
## # Transform into matrix of samples (cell lines) by features
## expression.data <- t(expression.data)
## colnames(expression.data) <- genes

# Check for NA values
sum(is.na(expression.data))

##  Convert expression data to log2 scale
expression.data <- expression.data %>%
    mutate(across(where(is.numeric), ~ .x + 1)) %>%
    log2()



# Remove duplicate gene names
##expression.data <- expression.data %>% 
##   select(-duplicated(names(expression.data)))


```

<a name="feature_selection"/>

### Feature selection

Selecting relevant and predictive features, or filtering out uninformative features, can often improve the speed and accuracy of a model. For now, we will demonstrate a simple filtering step.

```{r}
## function to find coefficient of variation
CV <- function(vec){
  return(sd(vec)/mean(vec))
}

## function for ranking genes by variance or cv
## inputs: data       data.frame      expression data with genes in rows and samples in columns
##         statistic  string          "variance" or "cv"
rankGenes <- function(data, statistic = "variance") {
    ranks <- data %>%
        filter(rownames(data) %in% c("A1BG_1", "A1BG-AS1_2")) %>%
        summarise(m = rowSums(.))
        


top.by.cv <- expression.data[,order(apply(expression.data, 2, CV), decreasing = T)][,1:500]
## Keep 5,000 genes with highest variance

### TODO: Summarize and compare top gene lists from by var and c.v. methods above ###
var.rank <- as.data.frame(cbind(gene = colnames(top.by.var),var_rank = 1:500))
cv.rank <- as.data.frame(cbind(gene = colnames(top.by.cv), cv_rank = 1:500))
ranks <- var.rank %>%
    full_join(cv.rank, by = "gene")


ggplot(ranks, aes(x = var_rank, y = cv_rank)) +
      geom_point()

```

<a name="imputation"/>

### Data imputation

To explore using imputation for missing data, we'll randomly delete a certain percentage of our sensitivity data and impute it. We will visualize the performance of the imputation with varying degrees of missing data by plotting the mean squared error (MSE).

$MSE = \frac{1}{n}\sum^{n}_{i=1}(\hat{Y}_i - Y_i)^2$


```{r, message = F, warning = F}

#source("http://bioconductor.org/biocLite.R")
#BiocManager::install("impute")

library(impute)

MSE <- function(yhat, y){
    squared.error <- sum(mapply(function(yhat,y) (yhat-y)^2, yhat, y))
    return(squared.error/length(yhat))
  }

# Initialize vectors for plotting
thresholds = c()
knn.errors = c()
min.errors = c()
median.errors = c()

for (percent in seq(.01, .50, by = .05)){

  expression.temp <- expression.data

  # Calculate the percentage of the data to delete
  obs.total <- length(expression.temp)
  col.sample <- sample(1:ncol(expression.temp), round(ncol(expression.temp)* percent), replace = FALSE) 
  row.sample <- sample(1:nrow(expression.temp), round(nrow(expression.temp)* percent), replace = FALSE)   
  # Store the deleted values before converting them to NAs
    deleted.values = mapply(function(row, col) 
                           return(expression.temp[row,col]), 
                    row = row.sample, col = col.sample)

    
  
  expression.temp[row.sample, col.sample] <- NA
  # Impute missing data with knn algorithm
  imputed.data <- impute.knn(as.matrix(expression.temp), k = round(nrow(expression.data)/10))
  imputed.values <- imputed.data$data[row.sample,col.sample]
  
### TODO: Impute missing data with 2 other algorithms ###
    

  imputed.min <- mapply(function(row, col) 
                           return(min(expression.temp[,col], na.rm = T)), 
                        row = row.sample, col = col.sample)
    imputed.median <- mapply(function(row, col) 
                           return(median(expression.temp[,col], na.rm = T)), 
                        row = row.sample, col = col.sample)
    
  
  
  # Evaluate performance with MSE
  mse <- MSE(imputed.values, deleted.values)
    min.mse <- MSE(imputed.min, deleted.values)
    median.mse <- MSE(imputed.median, deleted.values)
    
  thresholds = c(thresholds, percent)
    knn.errors = c(knn.errors, mse)
    min.errors = c(min.errors, min.mse)
    median.errors = c(median.errors, median.mse)

}

# Visualize MSE with differing proportions of missing data

errors <- as.data.frame(cbind(thresholds = thresholds,
                knn.errors = knn.errors,
                min.errors = min.errors,
                median.errors = median.errors))

  ggplot(errors, aes(x = thresholds, y = knn.errors)) +
  geom_line(color = "skyblue") +
      geom_line(aes(y = min.errors), color = "red") +
      geom_line(aes(y = median.errors), color = "green") +
  xlab("Proportion missing data") +
  ylab("Mean squared error") +
  theme_minimal()



### TODO: Add two more lines to the plot to visualize the new imputation methods ###
# Hint: can use lines(x, y, type = 'b') or plot() with the add = T parameter
# Use colors() to see list of possible colors in the base plotting system

```

<a name="ml"/>

### Machine learning

We have two necessary parts now: a cleaned data set of observed data ($X$) and a response vector ($Y$). In order to learn the coefficients ($\beta$) of a predictive model, we can use any number of supervised or unsupervised machine learning algorithms for classification or regression. Below are two simple regression examples.

```{r, message = F, warning = F}

# Can use caret::train, but we will discuss alternatives too
# install.packages("caret")
library(caret)

# Keep the cell lines that have values both for gene expression and response

cell.lines <- merge(top.by.var, meta[, c("LabID", "BMI")], by.x = 0, by.y = "LabID")
cell.lines <- cell.lines[!is.na(cell.lines$BMI),]
rownames(cell.lines) <- cell.lines$Row.names
cell.lines <- cell.lines[,-1]


combined.data <- cell.lines
# Split cell lines into training and test set
percent.training = .75
n = nrow(cell.lines)

inTrain <- sample(n, n * percent.training)
training <- combined.data[inTrain,]
testing  <- combined.data[-inTrain,]

# Train a linear model
lm.fit <- train(BMI ~ ., 
                data = training,
                method = 'lm')
lm.fit

# Test on our testing data set
#lm.preds <- predict(lm.fit, newdata = testing)
#mse.lm <- MSE(yhat = lm.preds, y = testing[,'Response'])

### TODO: try different training and test set sizes ###
# Think about other performance metrics

```

### Evaluation

With this data set, we are predicting on a continuous response. However, to demonstrate evaluation metrics for classification problems, we can instead imagine trying to classify cell lines depending on whether they are sensitive to the drug or not.

```{r, message = F}
# install.packages("ROCR")
library(ROCR)

# Summarize the drug response values
summary(drug7.response)

                                        # Define sensitive cell lines and convert response to binary
## Change to obesity
sensitive.cell.lines = rownames(cell.lines[cell.lines$BMI > 30,])
response = factor(row.names(expression.data) %in% sensitive.cell.lines)
table(response)

# Concatenate expression data and response vector for later
combined.data <- cbind(expression.data, Response = response)

# Re-split cell lines into training and test set
percent.training = .75
n = length(cell.lines)
inTrain <- sample(n, n * percent.training)
training <- combined.data[inTrain,]
testing  <- combined.data[-inTrain,]

# Classification with k-nearest neighbors
# (How could we pick a better k?)
knn.fit <- knn3(training, response[inTrain], k = 3)
knn.fit
knn.preds <- predict(knn.fit, newdata = testing, type = 'class')

# Confusion matrix
table(Predicted = knn.preds, Actual = response[-inTrain])
      
# Prediction probabilities of test data classes
knn.probs <- predict(knn.fit, newdata = testing, type = 'prob')[,2]
 # for an ROC curve there is a positive class (TRUE) - defining that class here
pred <- prediction(knn.probs, response[-inTrain])
perf <- performance(pred, 'tpr', 'fpr')
auc.perf <- unlist(performance(pred, 'auc')@y.values)
auc.report <- sprintf('AUC = %.2f', auc.perf)

# Prepare to display two plots side by side
par(mfrow = c(1,2))

# Plot TPR/FPR ROC with diagonal line and legend
plot(perf, main = 'ROC Curve', lwd = 2, col = 'red')
abline(a = 0, b= 1)
legend('bottomright', 
       legend = c('knn', auc.report), 
       lwd = c(2, 0),  
       col = c('red', 'white'))

# Plot precision/recall with legend
perf2 <- performance(pred, 'prec', 'rec')
plot(perf2, main = 'Precision/Recall Curve', lwd = 2, col = 'blue')
legend('bottomleft', legend = 'knn', lwd = 2,  col = 'blue')


```

```{r tidy = T}
sessionInfo()
```

